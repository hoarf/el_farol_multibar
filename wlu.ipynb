{
 "metadata": {
  "name": "",
  "signature": "sha256:0aa0bd6a44d2a3cf04ccaa1dfb2a526189549a84b1ba8621abff94a511ede91b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /vagrant_data/src/reinforcement-learning-practice/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/vagrant_data/src/reinforcement-learning-practice\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import math\n",
      "from matplotlib import pyplot as plt\n",
      "from copy import deepcopy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MAXREWARD = 1000.0\n",
      "GREEDY = 0\n",
      "EXPLORE = 1 \n",
      "ALPHA = .2\n",
      "MAX_METHOD = 0\n",
      "BAR_RESULT_BAD = 0\n",
      "BAR_RESULT_GOOD = 1\n",
      "ACTION_STAY_HOME = 0\n",
      "RANDOM_METHOD = 1\n",
      "DECAY = .9999\n",
      "\n",
      "class Agent:\n",
      "  \n",
      "  def __init__(self, maxactions):\n",
      "    \"\"\"\n",
      "    nr_bars: how many bar options the agent has\n",
      "    \"\"\"\n",
      "    self.choices = xrange(maxactions)\n",
      "    self.action_q_values = np.random.random_sample(maxactions)*MAXREWARD\n",
      "    self.p = 1\n",
      "\n",
      "  def __repr__(self):\n",
      "    \"\"\"\n",
      "    this makes a prettier print(agent)\n",
      "    \"\"\"\n",
      "    return repr(self.action_q_values)\n",
      "  \n",
      "  def chose_action(self, step):\n",
      "    \"\"\"\n",
      "    checks its Q-table and updates it's exploration probability based on a factor of decay\n",
      "    step: how many stepts the algorithm have run\n",
      "    \"\"\"\n",
      "    if np.random.choice([EXPLORE, GREEDY], p=[self.p,1-self.p]) == GREEDY:\n",
      "      best_actions = np.argwhere(self.action_q_values == np.max(self.action_q_values)).flatten()\n",
      "      self.action = np.random.choice(best_actions)\n",
      "    else:\n",
      "      self.action = np.random.choice(self.choices)\n",
      "        \n",
      "    self.p = self.p * (DECAY ** step)\n",
      "  \n",
      "  def update_utilities(self, reward):        \n",
      "    \"\"\"\n",
      "    takes the rewards vector and updates the Q-table\n",
      "    \"\"\"\n",
      "    q_a = self.action_q_values[self.action]\n",
      "    m = np.max(self.action_q_values)\n",
      "    self.action_q_values[self.action] = q_a + ALPHA*(reward + m - q_a)\n",
      "    \n",
      "class World:\n",
      "\n",
      "  def __init__(self, thresholds=[0.3, 0.5], nr_agents=100):\n",
      "    \"\"\"\n",
      "    nr_agents: how many agents there are in the world\n",
      "    thresholds: the optimum amount of ocuppation in a given bar\n",
      "    \"\"\"\n",
      "    self.thresholds = thresholds\n",
      "    self.maxbar = len(thresholds)\n",
      "    self.maxactions = self.maxbar + 1\n",
      "    self.attendences = np.zeros(self.maxactions) \n",
      "    self.agents = [ Agent(self.maxactions) for x in xrange(nr_agents) ]\n",
      "    self.week = 0\n",
      "    self.reward_function = self.get_reward_discrete if nr_agents <= 100 else self.get_reward\n",
      "\n",
      "  def __repr__(self):\n",
      "    \"\"\"\n",
      "    this makes a prettier print(world)\n",
      "    \"\"\"\n",
      "    return repr(self.agents)\n",
      "    \n",
      "  def get_reward(self, attendance, threshold):\n",
      "    \"\"\"\n",
      "    attendance: list of attendance counts for each bar\n",
      "    threshold: list of thresholds preferences for each bar\n",
      "    returns: agent's reward as a real value\n",
      "    \"\"\"\n",
      "    a = ((np.float128(attendance)/len(self.agents)-np.float128(threshold))**2)*MAXREWARD #DIFF\n",
      "    b = a**2\n",
      "    c = MAXREWARD/np.exp(b)\n",
      "    return c\n",
      "    \n",
      "  def get_reward_discrete(self, attendance, threshold):\n",
      "    \"\"\"\n",
      "    attendance: list of attendance counts for each bar\n",
      "    threshold: list of thresholds preferences for each bar\n",
      "    returns: agent's reward as a real value, but using a discrete version of the function get_reward()\n",
      "    \"\"\"\n",
      "    if attendance == threshold:\n",
      "      return MAXREWARD\n",
      "    elif (threshold - 0.1 <= attendance) or (attendance <= threshold + 0.1):\n",
      "      return 679.0\n",
      "    else:\n",
      "      return 0\n",
      "  \n",
      "  def calculate_world_utility(self, agent_set):\n",
      "    \"\"\"\n",
      "    missing_agent_from_bar: the index of the bar on wich we want to simulate one less agent\n",
      "    returns: the world utility (average utility over all agents)\n",
      "    \"\"\"\n",
      "    self.attendences = np.zeros(self.maxactions) \n",
      "    self.bar_results = np.zeros(self.maxbar) \n",
      "    self.rewards = np.zeros(self.maxactions) \n",
      "    \n",
      "    nr_agents = len(agent_set)\n",
      "\n",
      "    for agent in agent_set:\n",
      "      self.attendences[agent.action] += 1\n",
      "    \n",
      "    for bar in xrange(self.maxbar):\n",
      "      self.bar_results[bar] = BAR_RESULT_GOOD if self.attendences[bar+1]/nr_agents <= self.thresholds[bar] else BAR_RESULT_BAD\n",
      "    \n",
      "    home_good = np.count_nonzero(self.bar_results) == 0 #Stay in home is good if the result in all bars was bad (0)\n",
      "    \n",
      "    for action in xrange(self.maxactions):\n",
      "      if action == ACTION_STAY_HOME:\n",
      "        self.rewards[action] = MAXREWARD/self.maxbar if home_good else 0\n",
      "      else:\n",
      "        self.rewards[action] = self.bar_results[action-1]*self.reward_function(self.attendences[action], self.thresholds[action-1])\n",
      "    \n",
      "    return np.mean([self.rewards[a.action] for a in agent_set])\n",
      "  \n",
      "  def step(self):\n",
      "    \"\"\"\n",
      "    performs a time step and updates the world\n",
      "    \"\"\"\n",
      "    \n",
      "    for agent in self.agents:\n",
      "      agent.chose_action(self.week)\n",
      "      \n",
      "    self.G = self.calculate_world_utility(self.agents)\n",
      "    \n",
      "    reserva = self.agents.pop(0)\n",
      "    for agent in self.agents:\n",
      "      agent.update_utilities(self.G - self.calculate_world_utility(self.agents))\n",
      "      self.agents.append(reserva)\n",
      "      reserva = self.agents.pop(0)\n",
      "    \n",
      "    self.agents.append(reserva)\n",
      "    self.week += 1\n",
      "    \n",
      "      \n",
      "def ElFarolWLU(wks=5000, t=[0.3,0.5], a=100):\n",
      "  \"\"\"\n",
      "  wks: number of weeks as an integer\n",
      "  t: list of thresholds for each bar\n",
      "  a: number of agents as an integer\n",
      "  \"\"\"\n",
      "  weeks = np.zeros((len(t)+1,wks,1))\n",
      "  w = World(thresholds=t, nr_agents=a)\n",
      "  world_utilities = np.zeros(wks)\n",
      "  for week in xrange(wks-1):\n",
      "    w.step()\n",
      "    world_utilities[week] = w.G\n",
      "    for ix in xrange(len(t)+1):\n",
      "      weeks[ix][w.week] = w.attendences[ix]/len(w.agents)\n",
      "    \n",
      "  print \"Done week %i\" % w.week\n",
      "  return weeks, world_utilities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_attendances(maxbars, nr_weeks, data):\n",
      "  \"\"\"\n",
      "  maxbars: home many bars there are as an integer\n",
      "  nr_weeks: how many weeks has it run as an integer\n",
      "  data: matrix of NUMBER_OF_BARS x NUMBER_OF_WEEKS x 1\n",
      "  \"\"\"\n",
      "  x = linspace(0,nr_weeks-1,nr_weeks)\n",
      "  f, axis = plt.subplots(nrows=maxbars)\n",
      "  for index, ax in enumerate(axis):\n",
      "    ax.scatter(x, [data[index][k] for k in x], s=1)\n",
      "    ax.set_title(\"bar\" + str(index) if index != 0 else \"stay\")\n",
      "    ax.set_ylim(0,1)\n",
      "    ax.set_xlim(0,nr_weeks)\n",
      "    ax.set_xticks(np.arange(0,nr_weeks,nr_weeks/10))\n",
      "    ax.set_ylabel(\"Attendance\")\n",
      "    ax.set_xlabel(\"Weeks\")\n",
      "    ax.set_yticks(np.arange(0,1,.1))\n",
      "\n",
      "  f.set_size_inches(10.5, 10.1)\n",
      "  f.tight_layout()\n",
      "  \n",
      "def plot_world_utilities(nr_weeks, world_utilities):\n",
      "  \"\"\"\n",
      "  nr_weeks: how many weeks has it run as an integer\n",
      "  data: matrix of NUMBER_OF_BARS x NUMBER_OF_WEEKS x 1\n",
      "  \"\"\"\n",
      "  axis = plt.subplot(111)\n",
      "  x = linspace(0,nr_weeks-1,nr_weeks)\n",
      "  axis.scatter(x, world_utilities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wks = 1000\n",
      "t = [0.3,0.5]\n",
      "weeks, world_utilities = ElFarolWLU(wks, t=t, a=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_attendances(len(t)+1, wks, weeks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_world_utilities(wks, world_utilities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}