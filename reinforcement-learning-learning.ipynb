{
 "metadata": {
  "name": "",
  "signature": "sha256:388e69b8e0e4b3a178bde44c85133e3d4456100b1f9a465bb5d406412f3a3f6f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'The Book': This refers to the book: Russell, Stuart, and Peter Norvig. \"Artificial intelligence: a modern approach.\" (1995).\n",
      "\n",
      "State: State here is read as the index of a matrix. For instance in the Q matrix the position Q[0,1] corresponds to all the Q-values associated wit the state (0,1) for any given action. **Note that all the index here are 0-based as opposed to 1-based index in the book, also the book uses the notation (column, line) witch is confusing IMO so here (in the environ files) i'm using (line, column)**\n",
      "\n",
      "Action: This is one of the four possible choices for the agent at any given state, namely: LEFT, TOP, RIGHT, BOTTOM that will be represented in the algorith by the index 0, 1, 2 and 3 respectivelly "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reads the environment data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# The reward of the environment. For this experiment is either -0.04 for any valid non terminal \n",
      "# state and -1 and 1 for the terminal ones.\n",
      "rewards = pd.read_csv('/vagrant_data/src/reinforcement-learning-practice/rewards.csv', index_col=[0,1])\n",
      "\n",
      "# Basically is a list of valid actions. Don't really need to be file but.. meh, I prefer using my \n",
      "# regular text editor to edit large chunks of data\n",
      "q_values = pd.read_csv('/vagrant_data/src/reinforcement-learning-practice/q_values.csv', index_col=[0,1,2])\n",
      "\n",
      "# This is the expected utility written in the book, achived by faries\n",
      "utility = pd.read_csv('/vagrant_data/src/reinforcement-learning-practice/utility_values.csv', index_col=[0,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A percept represents an environment stimulus\n",
      "START_PERCEPT = {\n",
      "  'state': (0,0),\n",
      "  'reward': rewards.ix[0,0]['reward'],\n",
      "  'terminal': False\n",
      "}\n",
      "\n",
      "# Optimistic reward estimation\n",
      "R_PLUS = 2\n",
      "\n",
      "# At least how many times each state should be tried\n",
      "N_E = 5\n",
      "\n",
      "# Learning rate\n",
      "# 'n' is how many times that particular state-action pair have been tried\n",
      "def ALPHA(n):\n",
      "  return 1.0/n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A few helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# F: State, Action -> State\n",
      "# Calculates the next state based on the previous state plus the action\n",
      "def calculate_new_state_based_on_action(state, action):\n",
      "  state = (state[0]  , state[1]-1) if action[0] == 0 else state\n",
      "  state = (state[0]+1, state[1]  ) if action[0] == 1 else state\n",
      "  state = (state[0]  , state[1]+1) if action[0] == 2 else state\n",
      "  state = (state[0]-1, state[1]  ) if action[0] == 3 else state\n",
      "  return state\n",
      "\n",
      "# PICS OR DIDNT HAPPN\n",
      "pretty_action = {\n",
      "  0: '<-',\n",
      "  1: '^',\n",
      "  2: '->',\n",
      "  3: 'V',\n",
      "  4: 'NONE'\n",
      "}    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Initializations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A table containing the action-value pairs\n",
      "# Domain is State-index0 x State-index1 x Action\n",
      "Q = q_values.copy()\n",
      "\n",
      "# A table containing the state-action pairs\n",
      "# Domain is State-index0 x State-index1 x Action\n",
      "N = q_values.copy()\n",
      "\n",
      "# Last action taken\n",
      "a = None\n",
      "\n",
      "# Previous state\n",
      "i = None\n",
      "\n",
      "# Previous state's reward\n",
      "r = None\n",
      "\n",
      "# 'SOMETHING SOMETHING MADNESS SOMETHING SOMETHING DIFFERENT RESULTS' - Einstein\n",
      "np.random.seed = 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The main thing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the main algorithm that decides the new action for the agent for a given percept\n",
      "def Q_learn_agent(percept):\n",
      "  global i, a, r, Q, N\n",
      "  j = percept['state']\n",
      "  if percept['terminal']:\n",
      "    Q.ix[i].ix[4] = percept['reward'] \n",
      "  if i:\n",
      "    N.ix[i].ix[a] += 1\n",
      "    Q.ix[i].ix[a] = Q.ix[i].ix[a] + ALPHA(N.ix[i].ix[a])*(r + np.max(Q.ix[j]) - Q.ix[i].ix[a])    \n",
      "  i = j\n",
      "  r = percept['reward']\n",
      "  a = choose_action(j)\n",
      "  return a\n",
      "\n",
      "# The exploratory function written in the literature, it decides wheter or not we already \n",
      "# explored enough and are ready to be greedy\n",
      "def exploratory_fuction(u, n):\n",
      "  return R_PLUS if n < N_E else u\n",
      "\n",
      "# This makes our exploratory function accept a vector instead of a scalar\n",
      "v_exploratory_function = np.vectorize(exploratory_fuction)\n",
      "\n",
      "# At state: 'j' choses something to do.\n",
      "def choose_action(j):\n",
      "  exploration_corrected_q_value = v_exploratory_function(Q.ix[j], N.ix[j])    \n",
      "  # This returns an array with True as value if the position in the array is maximum. \n",
      "  # Ex: [False True True False False] for the utilities: [ 0 2 2 1 .5 ]\n",
      "  all_max_mask = exploration_corrected_q_value == np.max(exploration_corrected_q_value)     \n",
      "  # This maps the previous array to the relative frequency of True values. Basicaly I'm giving equal probabability of \n",
      "  # being randomly chosen for each max value on the utility array, I.E Choose one of the best actions randomly\n",
      "  action_frequencies = map(lambda x: 1.0/np.count_nonzero(all_max_mask) if x else 0, all_max_mask)\n",
      "  # This choses an action represented by the range(0,5) given the probabilities set previously.   \n",
      "  chosen_action = np.random.choice(Q.ix[j].index.values, 1, p=action_frequencies)  \n",
      "  return chosen_action\n",
      "\n",
      "# This is the environment deciding what feedback to give to the agent\n",
      "def new_percept(new_action):\n",
      "  new_state = calculate_new_state_based_on_action(i, new_action)\n",
      "  return {\n",
      "    'state': new_state,\n",
      "    'reward': rewards.ix[new_state]['reward'],\n",
      "    'terminal': new_state == (1,3) or new_state == (2,3)\n",
      "  }\n",
      "\n",
      "# This is where the whole process start\n",
      "def learn(nepochs=300):\n",
      "  global start_state_utility\n",
      "  for epoch in xrange(1,nepochs):\n",
      "    current_percept = START_PERCEPT\n",
      "    while True:\n",
      "      new_action = Q_learn_agent(current_percept)\n",
      "      if new_action == 4: break # means we reached a terminal state, let's start over a new epoch\n",
      "      np = new_percept(new_action)\n",
      "      current_percept = np   \n",
      "  print(\"Done. Everything Now.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nepoch = 50\n",
      "learn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done. Everything Now.\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Utility of all the states \n",
      "\n",
      "### Expected (according to the book): "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utility"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>q_value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>s0</th>\n",
        "      <th>s1</th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.705</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.655</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.611</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0.388</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.762</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.660</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-1.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.812</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.868</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.918</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1.000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "       q_value\n",
        "s0 s1         \n",
        "0  0     0.705\n",
        "   1     0.655\n",
        "   2     0.611\n",
        "   3     0.388\n",
        "1  0     0.762\n",
        "   2     0.660\n",
        "   3    -1.000\n",
        "2  0     0.812\n",
        "   1     0.868\n",
        "   2     0.918\n",
        "   3     1.000"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Actual"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "empirical_utility = pd.DataFrame(Q.groupby(level=['s0','s1'])['q_value'].max())\n",
      "empirical_utility"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>q_value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>s0</th>\n",
        "      <th>s1</th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.433699</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>-0.062485</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.065559</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-0.090943</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.621554</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.467971</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-1.068679</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
        "      <th>0</th>\n",
        "      <td> 0.783511</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.929457</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 1.085615</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 1.271198</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "        q_value\n",
        "s0 s1          \n",
        "0  0   0.433699\n",
        "   1  -0.062485\n",
        "   2   0.065559\n",
        "   3  -0.090943\n",
        "1  0   0.621554\n",
        "   2   0.467971\n",
        "   3  -1.068679\n",
        "2  0   0.783511\n",
        "   1   0.929457\n",
        "   2   1.085615\n",
        "   3   1.271198"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Difference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "empirical_utility.subtract(utility).plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<matplotlib.axes.AxesSubplot at 0x7fc450f69890>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEfCAYAAAC6Z4bJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fc450f69350>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The book is not that clear on which reward it uses for each case and don't say right away witch value of alpha it has chosen, just that it 'decays over time proportionally to the frequency an action-state pair is used'. Also Q-values just aproximate utilities and only converge in the long run so therefore theese are consitent results?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}