{
 "metadata": {
  "name": "",
  "signature": "sha256:0f21d272c31eaea397c2bb5cd03bb8ef8853ceccc7b235ecf0c0a2f4bde95df0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "'The Book': This refers to the book: Russell, Stuart, and Peter Norvig. \"Artificial intelligence: a modern approach.\" (1995).\n",
      "\n",
      "State: State here is read as the index of a matrix. For instance in the Q matrix the position Q[0,1] corresponds to all the Q-values associated wit the state (0,1) for any given action. **Note that all the index here are 0-based as opposed to 1-based index in the book**\n",
      "\n",
      "Action: This is one of the five possible choices for the agent at any given state, namely: LEFT, TOP, RIGHT, BOTTOM, STAND_STILL, that will be represented in the algorith by the index 0,1,2,3,4 respectivelly"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reads the evironment data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rewards_data = pd.read_csv('/vagrant_data/src/reinforcement-learning-practice/rewards.csv', index_col=[0,1])\n",
      "initial_utility_data = pd.read_csv('/vagrant_data/src/reinforcement-learning-practice/initial_utility_data.csv', index_col=[0,1,2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Converts the data into a more mathy format\n",
      "\n",
      "This enables the handling of a state as the index of a matrix, I.E rewards[1,0] corresponds to the reward associated with the state (1,0) which is convenient and makes the syntax more like the one in the book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rewards = rewards_data.as_matrix().reshape((3,4))\n",
      "initial_utility = initial_utility_data.as_matrix().reshape((3,4,5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is constant, a percept represents an environment stimulus\n",
      "START_PERCEPT = {\n",
      "  'state': (0,0),\n",
      "  'reward': rewards[0,0],\n",
      "  'terminal': False\n",
      "}\n",
      "\n",
      "# Optimistic reward estimation\n",
      "R_PLUS = 2\n",
      "\n",
      "# At least how many times each state should be tried\n",
      "N_E = 5\n",
      "\n",
      "# Learning rate\n",
      "alpha = .1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Initializations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A table containing the action-value pairs\n",
      "# Domain is State-index0 x State-index1 x Action\n",
      "Q = initial_utility\n",
      "\n",
      "# A table containing the state-action pairs\n",
      "# Domain is State-index0 x State-index1 x Action\n",
      "N = np.zeros((3,4,5))\n",
      "\n",
      "# Last action taken\n",
      "a = None\n",
      "\n",
      "# Previous state\n",
      "i = None\n",
      "\n",
      "# Previous state's reward\n",
      "r = None\n",
      "\n",
      "# 'SOMETHING SOMETHING MADNESS SOMETHING SOMETHING DIFFERENT RESULTS' - Einstein\n",
      "np.random.seed = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The main thing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the main algorithm that decides the new action for the agent for a given percept\n",
      "def Q_learn_agent(percept):\n",
      "  global i, a, r, Q, N\n",
      "  j = percept['state']\n",
      "  if i:\n",
      "    N[i][a] += 1\n",
      "    Q[i][a] = Q[i][a] + alpha*(r + np.max(Q[j]) - Q[i][a])\n",
      "  if percept['terminal']:\n",
      "    i = None\n",
      "  else:\n",
      "    i = j\n",
      "    r = percept['reward']\n",
      "  a = choose_action(j)\n",
      "  return a\n",
      "\n",
      "# The exploratory function written in the literature, it decides wheter or not we already explored enough and are ready to be greedy\n",
      "def exploratory_fuction(u, n):\n",
      "  return u if u == -9999 else R_PLUS if n < N_E else u\n",
      "\n",
      "# This makes our exploratry_function accept vectors as argument instead of scalars\n",
      "v_exploratory_function = np.vectorize(exploratory_fuction)\n",
      "\n",
      "# At state: 'j' choses something to do.\n",
      "def choose_action(j):\n",
      "  utility = v_exploratory_function(Q[j],N[j])\n",
      "  \n",
      "  # This returns an array with True as value if the position in the array is maximum. \n",
      "  #  Ex: [False True True False False] for the utilities: [ 0 2 2 1 .5]\n",
      "  all_max_mask = utility == np.max(utility) \n",
      "\n",
      "  # This maps the previous array to the relative frequency of True values. Basicaly I'm giving equal probabability of \n",
      "  # being randomly chosen for each max value on the utility array\n",
      "  action_frequencies = map(lambda x: 1.0/count_nonzero(all_max_mask) if x else 0, all_max_mask)\n",
      "   \n",
      "  # This choses an action represented by the range(0,5) given the probabilities set previously.   \n",
      "  return np.random.choice(range(0,5),1,p=action_frequencies)\n",
      "\n",
      "# This is the environment deciding what feedback to give to the agent\n",
      "def new_percept(new_action):\n",
      "  new_state = calculate_new_state_based_on_action(i, new_action)\n",
      "  return {\n",
      "    'state': new_state,\n",
      "    'reward': rewards[new_state],\n",
      "    'terminal': new_state == (1,3) or new_state == (2,3)\n",
      "  }\n",
      "\n",
      "# This calculates the coordinates of the new state based on the previous state plus the action\n",
      "# Action LEFT=0 ; TOP=1; RIGHT=2; BOTTOM=3; STANT_STILL=4;\n",
      "def calculate_new_state_based_on_action(state, action):\n",
      "  state = (state[0]  , state[1]-1) if action[0] == 0 else state\n",
      "  state = (state[0]+1, state[1]  ) if action[0] == 1 else state    \n",
      "  state = (state[0]  , state[1]+1) if action[0] == 2 else state\n",
      "  state = (state[0]-1, state[1]  ) if action[0] == 3 else state\n",
      "  return state\n",
      "\n",
      "# this is where the whole process start\n",
      "start_state_utility = []\n",
      "def learn(nepochs=300):\n",
      "  global start_state_utility, debug, last_actions\n",
      "  for epoch in xrange(1,nepochs):\n",
      "    print \"Epoch %i - Done.\" % epoch\n",
      "    current_percept = START_PERCEPT\n",
      "    start_state_utility.append(Q[0,0])\n",
      "    while True:\n",
      "      new_action = Q_learn_agent(current_percept)\n",
      "      if not i: # means we reached a terminal state, let's start over a new epoch\n",
      "        break\n",
      "      np = new_percept(new_action)\n",
      "      current_percept = np\n",
      "  print(\"Done. Everything Now.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nepoch = 10\n",
      "learn(nepoch)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch 1 - Done.\n",
        "Epoch 2 - Done.\n",
        "Epoch 3 - Done.\n",
        "Epoch 4 - Done.\n",
        "Epoch 5 - Done."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 6 - Done.\n",
        "Epoch 7 - Done.\n",
        "Epoch 8 - Done.\n",
        "Epoch 9 - Done.\n",
        "Done. Everything Now.\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Utility of start state (0,0) over the epochs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(xrange(1,nepoch), start_state_utility, label=range(0,5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "[<matplotlib.lines.Line2D at 0x7f2910ec3290>,\n",
        " <matplotlib.lines.Line2D at 0x7f2910ec3510>,\n",
        " <matplotlib.lines.Line2D at 0x7f2910ec3750>,\n",
        " <matplotlib.lines.Line2D at 0x7f2910ec3910>,\n",
        " <matplotlib.lines.Line2D at 0x7f2910ec3ad0>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f2911ce8450>"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Q.reshape(5,3,4) # The utility for every action, from 0 to 4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[[ -9.99900000e+03  -1.72073800e-02   2.87620598e-02  -9.99900000e+03]\n",
        "  [ -9.99900000e+03   3.51532198e-02  -9.99900000e+03   1.43229087e-01]\n",
        "  [ -9.99900000e+03  -9.99900000e+03   1.13425938e-01   7.91950949e-02]]\n",
        "\n",
        " [[  2.93752808e-01  -9.99900000e+03  -9.99900000e+03   4.77251958e-01]\n",
        "  [  3.43900000e-01  -9.99900000e+03  -9.99900000e+03  -9.99900000e+03]\n",
        "  [ -9.99900000e+03  -1.64600000e-02  -9.99900000e+03  -5.69859400e-02]]\n",
        "\n",
        " [[ -9.99900000e+03  -9.99900000e+03  -9.99900000e+03  -9.99900000e+03]\n",
        "  [ -9.99900000e+03  -9.99900000e+03  -9.99900000e+03  -8.41700000e-02]\n",
        "  [ -1.52347700e-01  -3.29574062e-02  -9.99900000e+03  -9.99900000e+03]]\n",
        "\n",
        " [[ -9.99900000e+03  -9.99900000e+03  -9.99900000e+03   0.00000000e+00]\n",
        "  [ -9.99900000e+03  -9.99900000e+03  -2.91100000e-02   0.00000000e+00]\n",
        "  [ -9.99900000e+03   0.00000000e+00  -9.99900000e+03  -4.17700000e-02]]\n",
        "\n",
        " [[ -9.99900000e+03  -9.99900000e+03   0.00000000e+00  -9.99900000e+03]\n",
        "  [ -5.44300000e-02  -1.03417000e-01  -9.99900000e+03  -9.99900000e+03]\n",
        "  [ -9.99900000e+03  -9.99900000e+03  -9.99900000e+03   0.00000000e+00]]]\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print N.reshape(5,3,4) # The frequency for every action from 0 to 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[[  0.   5.  13.   0.]\n",
        "  [  0.   5.   0.  13.]\n",
        "  [  0.   0.   5.   5.]]\n",
        "\n",
        " [[  9.   0.   0.   5.]\n",
        "  [  4.   0.   0.   0.]\n",
        "  [  0.   1.   0.   4.]]\n",
        "\n",
        " [[  0.   0.   0.   0.]\n",
        "  [  0.   0.   0.   2.]\n",
        "  [  4.   1.   0.   0.]]\n",
        "\n",
        " [[  0.   0.   0.   0.]\n",
        "  [  0.   0.   1.   0.]\n",
        "  [  0.   0.   0.   1.]]\n",
        "\n",
        " [[  0.   0.   0.   0.]\n",
        "  [  1.   2.   0.   0.]\n",
        "  [  0.   0.   0.   0.]]]\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Q[:,:,4] # Utility for every state to go to bottom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-9999. -9999. -9999. -9999.]\n",
        " [-9999. -9999. -9999.     0.]\n",
        " [-9999. -9999. -9999.     0.]]\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Q[0,0,:] # Utility for state (0,0) for every action"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ -9.99900000e+03  -1.72073800e-02   2.87620598e-02  -9.99900000e+03\n",
        "  -9.99900000e+03]\n"
       ]
      }
     ],
     "prompt_number": 141
    }
   ],
   "metadata": {}
  }
 ]
}